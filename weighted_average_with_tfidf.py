# weighted_average_with_tfidf.py

import numpy as np
import joblib
from sklearn.decomposition import SparseCoder
import sentencepiece as spm
from sklearn.feature_extraction.text import TfidfVectorizer
from scipy.special import softmax

# 1. æº–å‚™: ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
embedding_matrix = np.load("embedding_matrix.npy")
dictionary = joblib.load("dict_components.pkl")
sp = spm.SentencePieceProcessor()
sp.load("sp.model")

# 2. TF-IDF Vectorizer ã®æº–å‚™ï¼ˆcorpus.txt ã‚’ä½¿ã£ã¦ SentencePiece ãƒˆãƒ¼ã‚¯ãƒ³ã§å­¦ç¿’ï¼‰
with open("corpus.txt", "r", encoding="utf-8") as f:
    corpus_lines = [line.strip() for line in f.readlines() if line.strip()]

corpus_tokenized = [" ".join(sp.encode(line, out_type=str)) for line in corpus_lines]
tfidf_vectorizer = TfidfVectorizer(tokenizer=lambda x: x.split(), lowercase=False)
tfidf_vectorizer.fit(corpus_tokenized)

# Î±ã‚’è‡ªå‹•èª¿æ•´ã™ã‚‹é–¢æ•°ï¼ˆæ¡ˆA+Bã€ç¯„å›²èª¿æ•´ç‰ˆï¼‰
def estimate_alpha_hybrid(prompt, tfidf_weights):
    length = len(sp.encode(prompt))  # ãƒˆãƒ¼ã‚¯ãƒ³é•·
    tfidf_sum = tfidf_weights.sum()  # æƒ…å ±é‡

    # å€‹åˆ¥ã®Î±ï¼ˆå®‰å®šåŒ–ã®ãŸã‚åˆ†æ¯+1ã‚„Îµä»˜ãï¼‰
    alpha_length = 1.0 / (length + 1)
    alpha_tfidf = 1.0 / (tfidf_sum + 1e-8)

    # é‡ã¿ä»˜ãå¹³å‡ï¼ˆä¾‹ï¼šé•·ã•ã«0.4ã€TF-IDFã«0.6ã®é‡ã¿ï¼‰
    hybrid_alpha = 0.4 * alpha_length + 0.6 * alpha_tfidf

    # å®‰å®šåŒ–ã®ãŸã‚ã®ç¯„å›²åˆ¶é™ï¼ˆä¾‹: 0.2ã€œ0.8ï¼‰
    return max(0.2, min(0.8, hybrid_alpha))

# 4. TF-IDFåŠ é‡å¹³å‡ãƒ™ã‚¯ãƒˆãƒ«ã«ã‚ˆã‚‹äºˆæ¸¬é–¢æ•°ï¼ˆå‹•çš„Î±å¯¾å¿œï¼‰
def predict_next_token_tfidf(prompt):
    tokens = sp.encode(prompt, out_type=str)
    ids = sp.encode(prompt, out_type=int)

    if len(ids) == 0:
        return "<empty input>"

    vecs = embedding_matrix[ids]
    joined = " ".join(tokens)
    tfidf_weights = tfidf_vectorizer.transform([joined]).toarray().flatten()
    tfidf_weights = tfidf_weights[:len(vecs)]

    # sum ãŒ 0 ãªã‚‰ mean ã§ä»£æ›¿
    if tfidf_weights.sum() == 0:
        mean_vec = vecs.mean(axis=0).reshape(1, -1)
    else:
        tfidf_weights = tfidf_weights / (tfidf_weights.sum() + 1e-8)
        mean_vec = np.average(vecs, axis=0, weights=tfidf_weights).reshape(1, -1)

    # Î± ã‚’æ¨å®šã—ã¦ã‚¹ãƒ‘ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ€ã‚’åˆæœŸåŒ–
    alpha = estimate_alpha_hybrid(prompt, tfidf_weights)
    coder = SparseCoder(dictionary=dictionary, transform_algorithm='lasso_lars', transform_alpha=alpha)
    code = coder.transform(mean_vec)

    recon = np.dot(code, dictionary)
    similarities = np.dot(embedding_matrix, recon.T).squeeze()
    next_id = np.argmax(similarities)

    # softmaxç¢ºç‡ï¼ˆæ¡ˆBï¼‰
    probs = softmax(similarities)
    confidence = probs[next_id]

    # cosineæ­£è¦åŒ–ã‚¹ã‚³ã‚¢ï¼ˆæ¡ˆAï¼‰
    normed_sim = (similarities[next_id] - similarities.min()) / (similarities.max() - similarities.min() + 1e-8)

    # ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆæ¡ˆCï¼‰
    rank = similarities.argsort()[::-1].tolist().index(next_id) + 1

    # ğŸ“‹ è¤‡åˆè©•ä¾¡ãƒ­ã‚°å‡ºåŠ›
    print(f"  Î± used: {alpha:.4f}")
    print(f"  Cosine similarity (raw): {similarities[next_id]:.4f}")
    print(f"  Cosine similarity (normalized): {normed_sim:.4f}")
    print(f"  Confidence (softmax): {confidence:.4f}")
    print(f"  Token rank: {rank}")
    top5_ids = similarities.argsort()[-5:][::-1]
    top5_tokens = [sp.id_to_piece(int(i)) for i in top5_ids]
    print(f"  Top-5 similar tokens: {top5_tokens}")

    return sp.id_to_piece(int(next_id))

# 5. ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
def run_tests():
    prompts = [
        "The future of AI is",
        "Sparse modeling is",
        "I want to build",
        "My favorite language is",
        "Deep learning and sparse coding are"
    ]
    for prompt in prompts:
        print(f"Input: {prompt}")
        print("Next token:", predict_next_token_tfidf(prompt))
        print("---")

if __name__ == "__main__":
    run_tests()
